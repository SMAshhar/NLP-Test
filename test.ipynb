{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "# Building a pretrained VGG-16 feature extractor as encoder:\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "# We recover the feature maps returned by each of the 3 final blocks:\n",
    "f3 = vgg16.get_layer('block3_pool').output # shape: (28, 28, 256)\n",
    "f4 = vgg16.get_layer('block4_pool').output # shape: (14, 14, 512)\n",
    "f5 = vgg16.get_layer('block5_pool').output # shape: ( 7, 7, 512)\n",
    "# We replace the VGG dense layers by convs, adding the \"decoding\" layers\n",
    "instead after the conv/pooling blocks:\n",
    "f3 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f3)\n",
    "f4 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f4)\n",
    "f5 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f5)\n",
    "# We upscale `f5` to a 14x14 map so it can be merged with `f4`:\n",
    "f5x2 = Conv2DTranspose(filters=out_chh, kernel_size=4,strides=2,\n",
    "padding='same', activation='relu')(f5)\n",
    "# We merge the 2 feature maps with an element-wise addition:\n",
    "m1 = add([f4, f5x2])\n",
    "# We repeat the operation to merge `m1` and `f3` into a 28x28 map:\n",
    "m1x2 = Conv2DTranspose(filters=out_ch, kernel_size=4, strides=2,\n",
    "padding='same', activation='relu')(m1)\n",
    "m2 = add([f3, m1x2])\n",
    "# Finally, we use a transp-conv to recover the original shape:\n",
    "outputs = Conv2DTranspose(filters=out_ch, kernel_size=16, strides=8,\n",
    "padding='same', activation='sigmoid')(m2)\n",
    "fcn_8s = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'your', 'text', 'is', 'split', 'into', 'tokens,', 'you', 'need', 'to', 'encode', 'each', 'token', 'into', 'a', 'numerical', 'representation.', 'you', 'could', 'potentially', 'do', 'this', 'in', 'a', 'stateless', 'way,', 'such', 'as', 'by', 'hashing', 'each', 'token', 'into', 'a', 'fixed', 'binary', 'vector,', 'but', 'in', 'practice,', 'the', 'way', \"you'd\", 'go', 'about', 'it', 'is', 'to', 'build', 'an', 'index', 'of', 'all', 'terms', 'found', 'in', 'the', 'training', 'data', '(the', '“vocabulary”),', 'and', 'assign', 'a', 'unique', 'integer', 'to', 'each', 'entry', 'in', 'the', 'vocabulary.', 'something', 'like', 'this']\n",
      "{'once': 0, 'your': 1, 'text': 2, 'is': 3, 'split': 4, 'into': 5, 'tokens,': 6, 'you': 7, 'need': 8, 'to': 9, 'encode': 10, 'each': 11, 'token': 12, 'a': 13, 'numerical': 14, 'representation.': 15, 'could': 16, 'potentially': 17, 'do': 18, 'this': 19, 'in': 20, 'stateless': 21, 'way,': 22, 'such': 23, 'as': 24, 'by': 25, 'hashing': 26, 'fixed': 27, 'binary': 28, 'vector,': 29, 'but': 30, 'practice,': 31, 'the': 32, 'way': 33, \"you'd\": 34, 'go': 35, 'about': 36, 'it': 37, 'build': 38, 'an': 39, 'index': 40, 'of': 41, 'all': 42, 'terms': 43, 'found': 44, 'training': 45, 'data': 46, '(the': 47, '“vocabulary”),': 48, 'and': 49, 'assign': 50, 'unique': 51, 'integer': 52, 'entry': 53, 'vocabulary.': 54, 'something': 55, 'like': 56}\n"
     ]
    }
   ],
   "source": [
    "data =\"\"\"Once your text is split into tokens, you need to encode each token into a numerical\n",
    "representation. You could potentially do this in a stateless way, such as by hashing each\n",
    "token into a fixed binary vector, but in practice, the way you'd go about it is to build\n",
    "an index of all terms found in the training data (the “vocabulary”), and assign a\n",
    "unique integer to each entry in the vocabulary.\n",
    "Something like this\"\"\"\n",
    "data = data.replace(\"!\", \"\")\n",
    "data = data.replace(\"?\", \"\")\n",
    "data = data.lower().split()\n",
    "print(data)\n",
    "\n",
    "vocabulary = {}\n",
    "for word in data:\n",
    "    if word not in vocabulary:\n",
    "        vocabulary[word] = len(vocabulary)\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once\n",
      "your\n",
      "text\n",
      "is\n",
      "split\n",
      "into\n",
      "tokens,\n",
      "you\n",
      "need\n",
      "to\n",
      "encode\n",
      "each\n",
      "token\n",
      "a\n",
      "numerical\n",
      "representation.\n",
      "could\n",
      "potentially\n",
      "do\n",
      "this\n",
      "in\n",
      "stateless\n",
      "way,\n",
      "such\n",
      "as\n",
      "by\n",
      "hashing\n",
      "fixed\n",
      "binary\n",
      "vector,\n",
      "but\n",
      "practice,\n",
      "the\n",
      "way\n",
      "you'd\n",
      "go\n",
      "about\n",
      "it\n",
      "build\n",
      "an\n",
      "index\n",
      "of\n",
      "all\n",
      "terms\n",
      "found\n",
      "training\n",
      "data\n",
      "(the\n",
      "“vocabulary”),\n",
      "and\n",
      "assign\n",
      "unique\n",
      "integer\n",
      "entry\n",
      "vocabulary.\n",
      "something\n",
      "like\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "z = np.zeros((len(vocabulary),))\n",
    "for key in vocabulary.keys():\n",
    "    # z = np.hstack((z, one_hot_encode_token(key)))\n",
    "    print(key)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3306"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode=\"int\",)\n",
    "\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "text_vectorization = TextVectorization(output_mode=\"int\",standardize=custom_standardization_fn, split=custom_split_fn,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "\"I write, erase, rewrite\",\n",
    "\"Erase again, and then\",\n",
    "\"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'Big S isn\\'t playing with taboos or forcing an agenda like, say Mencia or Chapelle (though I like them both). She states the obvious in subtle, near subliminal remarks. Her show won\\'t change the World, nor is it meant to. But, along with the hilarious Brian Posehn and Paget Brewster\\'s ex-boyfriend Jay Johnston of \"Mr. Show\" fame, this is one mean show with an appetite for destruction! My side\\'s were thoroughly wrecked by the first episode. Look, I love this woman and like her famed boyfriend, Jimmy Kimmel, she just delivers the lines and lets the viewer run- with-it. The best kind of comedy around. Spoofing anything and anyone, like \"Mary Poppins\" in the second episode when she sings to the fake birds on to quick hitting commentary on society and college aged existential nonsense. This one is highly recommended, but only for those who still have a funny bone (and didn\\'t lose it in their most recent lippo-suction treatment or boob job).', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "                lambda x, y: (text_vectorization(x), y),\n",
    "                num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "                lambda x, y: (text_vectorization(x), y),\n",
    "                num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "                lambda x, y: (text_vectorization(x), y),\n",
    "                num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 20s 28ms/step - loss: 0.4881 - accuracy: 0.7987 - val_loss: 0.3110 - val_accuracy: 0.8902\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.3133 - accuracy: 0.8783 - val_loss: 0.3007 - val_accuracy: 0.8952\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.2730 - accuracy: 0.8945 - val_loss: 0.3188 - val_accuracy: 0.8948\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.2507 - accuracy: 0.9039 - val_loss: 0.3424 - val_accuracy: 0.8826\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.2414 - accuracy: 0.9051 - val_loss: 0.3612 - val_accuracy: 0.8820\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.2399 - accuracy: 0.9094 - val_loss: 0.3767 - val_accuracy: 0.8798\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2293 - accuracy: 0.9125 - val_loss: 0.3736 - val_accuracy: 0.8740\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2161 - accuracy: 0.9134 - val_loss: 0.3845 - val_accuracy: 0.8732\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.2190 - accuracy: 0.9111 - val_loss: 0.3696 - val_accuracy: 0.8766\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.2108 - accuracy: 0.9151 - val_loss: 0.3817 - val_accuracy: 0.8770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74c42ab220>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tfidf_2gram_train_ds.cache(),\n",
    "    validation_data=tfidf_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2890 - accuracy: 0.8973\n",
      "Test acc: 0.897\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
