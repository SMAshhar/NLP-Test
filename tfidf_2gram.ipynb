{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'your', 'text', 'is', 'split', 'into', 'tokens,', 'you', 'need', 'to', 'encode', 'each', 'token', 'into', 'a', 'numerical', 'representation.', 'you', 'could', 'potentially', 'do', 'this', 'in', 'a', 'stateless', 'way,', 'such', 'as', 'by', 'hashing', 'each', 'token', 'into', 'a', 'fixed', 'binary', 'vector,', 'but', 'in', 'practice,', 'the', 'way', \"you'd\", 'go', 'about', 'it', 'is', 'to', 'build', 'an', 'index', 'of', 'all', 'terms', 'found', 'in', 'the', 'training', 'data', '(the', '“vocabulary”),', 'and', 'assign', 'a', 'unique', 'integer', 'to', 'each', 'entry', 'in', 'the', 'vocabulary.', 'something', 'like', 'this']\n",
      "{'once': 0, 'your': 1, 'text': 2, 'is': 3, 'split': 4, 'into': 5, 'tokens,': 6, 'you': 7, 'need': 8, 'to': 9, 'encode': 10, 'each': 11, 'token': 12, 'a': 13, 'numerical': 14, 'representation.': 15, 'could': 16, 'potentially': 17, 'do': 18, 'this': 19, 'in': 20, 'stateless': 21, 'way,': 22, 'such': 23, 'as': 24, 'by': 25, 'hashing': 26, 'fixed': 27, 'binary': 28, 'vector,': 29, 'but': 30, 'practice,': 31, 'the': 32, 'way': 33, \"you'd\": 34, 'go': 35, 'about': 36, 'it': 37, 'build': 38, 'an': 39, 'index': 40, 'of': 41, 'all': 42, 'terms': 43, 'found': 44, 'training': 45, 'data': 46, '(the': 47, '“vocabulary”),': 48, 'and': 49, 'assign': 50, 'unique': 51, 'integer': 52, 'entry': 53, 'vocabulary.': 54, 'something': 55, 'like': 56}\n"
     ]
    }
   ],
   "source": [
    "data =\"\"\"Once your text is split into tokens, you need to encode each token into a numerical\n",
    "representation. You could potentially do this in a stateless way, such as by hashing each\n",
    "token into a fixed binary vector, but in practice, the way you'd go about it is to build\n",
    "an index of all terms found in the training data (the “vocabulary”), and assign a\n",
    "unique integer to each entry in the vocabulary.\n",
    "Something like this\"\"\"\n",
    "data = data.replace(\"!\", \"\")\n",
    "data = data.replace(\"?\", \"\")\n",
    "data = data.lower().split()\n",
    "print(data)\n",
    "\n",
    "vocabulary = {}\n",
    "for word in data:\n",
    "    if word not in vocabulary:\n",
    "        vocabulary[word] = len(vocabulary)\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once\n",
      "your\n",
      "text\n",
      "is\n",
      "split\n",
      "into\n",
      "tokens,\n",
      "you\n",
      "need\n",
      "to\n",
      "encode\n",
      "each\n",
      "token\n",
      "a\n",
      "numerical\n",
      "representation.\n",
      "could\n",
      "potentially\n",
      "do\n",
      "this\n",
      "in\n",
      "stateless\n",
      "way,\n",
      "such\n",
      "as\n",
      "by\n",
      "hashing\n",
      "fixed\n",
      "binary\n",
      "vector,\n",
      "but\n",
      "practice,\n",
      "the\n",
      "way\n",
      "you'd\n",
      "go\n",
      "about\n",
      "it\n",
      "build\n",
      "an\n",
      "index\n",
      "of\n",
      "all\n",
      "terms\n",
      "found\n",
      "training\n",
      "data\n",
      "(the\n",
      "“vocabulary”),\n",
      "and\n",
      "assign\n",
      "unique\n",
      "integer\n",
      "entry\n",
      "vocabulary.\n",
      "something\n",
      "like\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "z = np.zeros((len(vocabulary),))\n",
    "for key in vocabulary.keys():\n",
    "    # z = np.hstack((z, one_hot_encode_token(key)))\n",
    "    print(key)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode=\"int\",)\n",
    "\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "text_vectorization = TextVectorization(output_mode=\"int\",standardize=custom_standardization_fn, split=custom_split_fn,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "\"I write, rewrite, and still rewrite again\"\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'rewrite', 'write', 'still', 'i', 'and', 'again']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 3 2 6 4 2 7], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and still rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "\"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"This is about a mad scientist who creates a half shark - half man type critter on an uncharted island, then calls up all his old business and academic buddies to come and see his creation (evil laugh) but actually he wants his sharkman to kill them! Lots of bad GCI, goofy plot elements, and babes sweating in tight T-shirts follow.<br /><br />These monster movies all follow a similar formula, but this one spices things up with a bit of humor (I guarantee the folks who made this had tongues firmly planted in cheek), not to mention the sexy babes. But let's mention those sexy babes - several hot babes, in tight T-shirts, sweating profusely. One's in her undies at the beginning, another at the end. Thank you, thank you, makers of bad movies! The plot is full of goofy stuff, a guy drives up in a jeep, slams it right into a tree, then offers to fly everyone off the island in a helicopter. Yeah, um, well, how 'bout we think about that for a while; we'll get back to you. The sharkman is hilarious - either awful CGI or an equally comedic guy in a rubber suit. The mad scientist gives a pretty good performance; he's evil, that's his motivation, he makes no apologies.<br /><br />Overall, if you want a stupid, FUN B-movie, this one should do the job.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "                lambda x, y: (text_vectorization(x), y),\n",
    "                num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "                lambda x, y: (text_vectorization(x), y),\n",
    "                num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "                lambda x, y: (text_vectorization(x), y),\n",
    "                num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 11s 16ms/step - loss: 0.4992 - accuracy: 0.7661 - val_loss: 0.3014 - val_accuracy: 0.8908\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3447 - accuracy: 0.8514 - val_loss: 0.5145 - val_accuracy: 0.7904\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.2967 - accuracy: 0.8763 - val_loss: 0.3739 - val_accuracy: 0.8656\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2719 - accuracy: 0.8907 - val_loss: 0.3578 - val_accuracy: 0.8824\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2508 - accuracy: 0.8985 - val_loss: 0.3490 - val_accuracy: 0.8864\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2453 - accuracy: 0.9016 - val_loss: 0.3411 - val_accuracy: 0.8862\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2337 - accuracy: 0.9040 - val_loss: 0.3479 - val_accuracy: 0.8876\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2288 - accuracy: 0.9056 - val_loss: 0.3975 - val_accuracy: 0.8834\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2212 - accuracy: 0.9040 - val_loss: 0.3782 - val_accuracy: 0.8826\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2170 - accuracy: 0.9068 - val_loss: 0.3912 - val_accuracy: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bb0446c50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tfidf_2gram_train_ds.cache(),\n",
    "    validation_data=tfidf_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 6ms/step - loss: 0.2999 - accuracy: 0.8853\n",
      "Test acc: 0.885\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
